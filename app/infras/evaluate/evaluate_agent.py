from langchain.callbacks.base import BaseCallbackHandler
from langchain_core.messages import BaseMessage, AIMessage, ToolMessage
from langchain_core.outputs import LLMResult
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import time
import json


class LogLevel(Enum):
    """æ—¥å¿—çº§åˆ«"""
    DEBUG = 0
    INFO = 1
    WARNING = 2
    ERROR = 3


@dataclass
class NodeExecution:
    """èŠ‚ç‚¹æ‰§è¡Œè®°å½•"""
    name: str
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    status: str = "running"
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    # æ–°å¢ï¼šèŠ‚ç‚¹è¾“å‡ºå†…å®¹
    output_message: Optional[str] = None  # èŠ‚ç‚¹è¾“å‡ºçš„æ¶ˆæ¯å†…å®¹
    output_data: Optional[Dict[str, Any]] = None  # èŠ‚ç‚¹è¾“å‡ºçš„ç»“æ„åŒ–æ•°æ®


@dataclass
class LLMExecution:
    """LLM è°ƒç”¨è®°å½•"""
    node_context: str  # æ¥è‡ªå“ªä¸ªèŠ‚ç‚¹
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    model: Optional[str] = None


@dataclass
class ToolExecution:
    """å·¥å…·è°ƒç”¨è®°å½•"""
    name: str
    node_context: str  # æ¥è‡ªå“ªä¸ªèŠ‚ç‚¹
    start_time: float
    end_time: Optional[float] = None
    duration: Optional[float] = None
    input_preview: Optional[str] = None
    output_preview: Optional[str] = None
    status: str = "running"
    error: Optional[str] = None


@dataclass
class RouterDecision:
    """è·¯ç”±å†³ç­–è®°å½•"""
    step: str
    decision: str
    reason: Optional[str] = None
    timestamp: float = field(default_factory=time.time)


@dataclass
class NodeOutput:
    """èŠ‚ç‚¹è¾“å‡ºè®°å½•"""
    node_name: str
    timestamp: float
    message_content: Optional[str] = None  # AI æ¶ˆæ¯å†…å®¹
    state_updates: Optional[Dict[str, Any]] = None  # çŠ¶æ€æ›´æ–°
    plans: Optional[List[Dict]] = None  # ç”Ÿæˆçš„æ–¹æ¡ˆ (plan èŠ‚ç‚¹)
    options: Optional[Dict[str, List]] = None  # æœç´¢ç»“æœé€‰é¡¹ (flights/hotels)


@dataclass
class WorkflowTrace:
    """å®Œæ•´çš„å·¥ä½œæµè¿½è¸ª"""
    session_id: str
    user_input: str
    start_time: float
    end_time: Optional[float] = None

    nodes: List[NodeExecution] = field(default_factory=list)
    llm_calls: List[LLMExecution] = field(default_factory=list)
    tool_calls: List[ToolExecution] = field(default_factory=list)
    router_decisions: List[RouterDecision] = field(default_factory=list)
    node_outputs: List[NodeOutput] = field(default_factory=list)  # æ–°å¢ï¼šèŠ‚ç‚¹è¾“å‡ºåˆ—è¡¨

    final_response: Optional[str] = None
    status: str = "running"  # running, completed, error
    error: Optional[str] = None


class AgentPerformanceMonitor(BaseCallbackHandler):
    """
    LangGraph Agent é«˜çº§æ€§èƒ½ç›‘æ§å›è°ƒå¤„ç†å™¨ã€‚

    åŠŸèƒ½ç‰¹æ€§:
    =========
    1. ğŸ” ç»†ç²’åº¦è¿½è¸ª: LLM è°ƒç”¨ã€å·¥å…·æ‰§è¡Œã€èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸ
    2. ğŸš¦ è·¯ç”±å†³ç­–ç›‘æ§: æ•è· Router çš„ step/decision å˜åŒ–
    3. ğŸ“Š è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š: Token æ¶ˆè€—ã€è€—æ—¶åˆ†æã€è°ƒç”¨é“¾è·¯
    4. ğŸ¨ ç¾åŒ–è¾“å‡º: å¯è‡ªå®šä¹‰æ—¥å¿—çº§åˆ«å’Œæ ¼å¼
    5. âš ï¸ é”™è¯¯è¿½è¸ª: å®Œæ•´çš„å¼‚å¸¸ä¸Šä¸‹æ–‡è®°å½•
    6. ğŸ“¤ æ•°æ®å¯¼å‡º: æ”¯æŒ JSON æ ¼å¼å¯¼å‡ºè¿½è¸ªæ•°æ®

    ä½¿ç”¨ç¤ºä¾‹:
    ========
    monitor = AgentPerformanceMonitor(
        log_level=LogLevel.INFO,
        show_tool_io=True,
        max_preview_length=200
    )
    result = await agent.ainvoke(inputs, config={"callbacks": [monitor]})
    monitor.print_summary()
    """

    def __init__(
        self,
        log_level: LogLevel = LogLevel.INFO,
        show_tool_io: bool = True,
        show_router_decisions: bool = True,
        max_preview_length: int = 150,
        session_id: Optional[str] = None,
        on_event: Optional[Callable[[str, Dict[str, Any]], None]] = None
    ):
        """
        åˆå§‹åŒ–ç›‘æ§å™¨ã€‚

        Args:
            log_level: æ—¥å¿—çº§åˆ« (DEBUG/INFO/WARNING/ERROR)
            show_tool_io: æ˜¯å¦æ˜¾ç¤ºå·¥å…·è¾“å…¥è¾“å‡ºé¢„è§ˆ
            show_router_decisions: æ˜¯å¦æ˜¾ç¤ºè·¯ç”±å†³ç­–
            max_preview_length: é¢„è§ˆæ–‡æœ¬çš„æœ€å¤§é•¿åº¦
            session_id: ä¼šè¯ ID (ç”¨äºè¿½è¸ª)
            on_event: äº‹ä»¶å›è°ƒå‡½æ•°ï¼Œç”¨äºå¤–éƒ¨ç³»ç»Ÿé›†æˆ
        """
        self.log_level = log_level
        self.show_tool_io = show_tool_io
        self.show_router_decisions = show_router_decisions
        self.max_preview_length = max_preview_length
        self.on_event = on_event

        # è¿½è¸ªæ•°æ®
        self.trace = WorkflowTrace(
            session_id=session_id or datetime.now().strftime("%Y%m%d_%H%M%S"),
            user_input="",
            start_time=0
        )

        # è¿è¡Œæ—¶çŠ¶æ€
        self._current_node: Optional[str] = None
        self._current_llm: Optional[LLMExecution] = None
        self._current_tool: Optional[ToolExecution] = None
        self._node_stack: List[str] = []  # èŠ‚ç‚¹è°ƒç”¨æ ˆ
        self._last_step: Optional[str] = None  # ä¸Šä¸€æ¬¡çš„ step å€¼

        # ç»Ÿè®¡ç´¯è®¡
        self.total_tokens = {"prompt": 0, "completion": 0, "total": 0}

        # éœ€è¦å¿½ç•¥çš„å†…éƒ¨èŠ‚ç‚¹å
        self._ignored_nodes = {
            "LangGraph", "RunnableSequence", "RunnableLambda",
            "ChannelWrite", "ChannelRead", "__start__", "__end__",
            "RunnableParallel", "RunnableAssign", "ChatPromptTemplate"
        }

        # å·²çŸ¥çš„ LangGraph èŠ‚ç‚¹å (ç”¨äºä» tags ä¸­è¯†åˆ«)
        self._known_nodes = {
            "intent_router", "collect", "plan", "search_flight", "select_flight",
            "pay_flight", "search_hotel", "select_hotel", "pay_hotel",
            "summary", "check_weather", "side_chat", "guide"
        }

    def _log(self, level: LogLevel, message: str, indent: int = 0):
        """ç»Ÿä¸€æ—¥å¿—è¾“å‡º"""
        if level.value >= self.log_level.value:
            prefix = "   " * indent
            print(f"{prefix}{message}")

    def _emit_event(self, event_type: str, data: Dict[str, Any]):
        """å‘é€äº‹ä»¶åˆ°å¤–éƒ¨ç³»ç»Ÿ"""
        if self.on_event:
            self.on_event(event_type, data)

    def _truncate(self, text: str) -> str:
        """æˆªæ–­è¿‡é•¿æ–‡æœ¬"""
        if len(text) > self.max_preview_length:
            return text[:self.max_preview_length] + "..."
        return text

    def _get_node_icon(self, node_name: str) -> str:
        """æ ¹æ®èŠ‚ç‚¹åç§°è·å–å›¾æ ‡"""
        icons = {
            "collect": "ğŸ“‹",
            "router": "ğŸš¦",
            "plan": "ğŸ“",
            "search_flight": "âœˆï¸",
            "search_hotel": "ğŸ¨",
            "select_flight": "ğŸ«",
            "select_hotel": "ğŸ›ï¸",
            "pay_flight": "ğŸ’³",
            "pay_hotel": "ğŸ’°",
            "check_weather": "ğŸŒ¤ï¸",
            "summary": "ğŸ“Š",
            "side_chat": "ğŸ’¬",
            "guide": "ğŸ—ºï¸",
        }
        return icons.get(node_name, "ğŸ“")

    # ===== Chain ç”Ÿå‘½å‘¨æœŸ =====

    def on_chain_start(
        self, serialized: Optional[Dict[str, Any]], inputs: Dict[str, Any], **kwargs: Any
    ) -> Any:
        """å½“ Chain (æˆ– Graph èŠ‚ç‚¹) å¼€å§‹è¿è¡Œæ—¶è§¦å‘"""

        # å·¥ä½œæµå¼€å§‹
        if not self.trace.start_time:
            self.trace.start_time = time.time()
            # å°è¯•æå–ç”¨æˆ·è¾“å…¥
            if "messages" in inputs and inputs["messages"]:
                first_msg = inputs["messages"][0]
                if hasattr(first_msg, "content"):
                    self.trace.user_input = first_msg.content
            self._log(LogLevel.INFO, f"\nğŸš€ [Monitor] Agent Workflow Started")
            self._emit_event("workflow_start", {"time": self.trace.start_time})

        # å¤šç§æ–¹å¼æå–èŠ‚ç‚¹åç§° (LangGraph å…¼å®¹)
        name = None

        # æ–¹å¼1: ä» kwargs ä¸­è·å– (LangGraph å¸¸ç”¨)
        if "name" in kwargs:
            name = kwargs["name"]

        # æ–¹å¼2: ä» tags ä¸­è·å– (LangGraph èŠ‚ç‚¹å¯èƒ½æ”¾åœ¨ tags é‡Œ)
        if not name and "tags" in kwargs:
            tags = kwargs["tags"]
            for tag in tags:
                if tag.startswith("graph:step:"):
                    name = tag.replace("graph:step:", "")
                    break
                # LangGraph èŠ‚ç‚¹åç§°é€šå¸¸æ˜¯ç®€å•å­—ç¬¦ä¸²
                if tag in self._known_nodes:
                    name = tag
                    break

        # æ–¹å¼3: ä» serialized ä¸­è·å–
        if not name and serialized:
            name = serialized.get("name")
            if not name:
                # å¯èƒ½æ˜¯ id åˆ—è¡¨çš„æœ€åä¸€ä¸ªå…ƒç´ 
                id_val = serialized.get("id")
                if isinstance(id_val, list) and id_val:
                    name = id_val[-1]
                elif isinstance(id_val, str):
                    name = id_val

        if not name or name in self._ignored_nodes:
            return

        # è®°å½•èŠ‚ç‚¹å¼€å§‹
        self._node_stack.append(name)
        self._current_node = name

        node_exec = NodeExecution(
            name=name,
            start_time=time.time(),
            metadata={"inputs_keys": list(
                inputs.keys()) if isinstance(inputs, dict) else []}
        )
        self.trace.nodes.append(node_exec)

        icon = self._get_node_icon(name)
        self._log(LogLevel.INFO, f"{icon} [Node] Entering: {name}", indent=0)
        self._emit_event(
            "node_start", {"name": name, "time": node_exec.start_time})

    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:
        """å½“ Chain ç»“æŸæ—¶è§¦å‘"""
        # ä» kwargs æå–èŠ‚ç‚¹åç§° (ä¸ on_chain_start ä¿æŒä¸€è‡´)
        name = kwargs.get("name")
        if not name and "tags" in kwargs:
            tags = kwargs["tags"]
            for tag in tags:
                if tag.startswith("graph:step:"):
                    name = tag.replace("graph:step:", "")
                    break
                if tag in self._known_nodes:
                    name = tag
                    break

        # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„èŠ‚ç‚¹åï¼Œæ›´æ–°è®°å½•å¹¶æ•è·è¾“å‡º
        if name and name in [n.name for n in self.trace.nodes]:
            for node in reversed(self.trace.nodes):
                if node.name == name and node.end_time is None:
                    node.end_time = time.time()
                    node.duration = node.end_time - node.start_time
                    node.status = "completed"

                    # æ•è·èŠ‚ç‚¹è¾“å‡º
                    if isinstance(outputs, dict):
                        self._capture_node_output(name, outputs, node)

                    # ä»æ ˆä¸­ç§»é™¤
                    if name in self._node_stack:
                        self._node_stack.remove(name)
                    break
        elif self._node_stack:
            # å›é€€é€»è¾‘: æŒ‰æ ˆé¡ºåºå¤„ç†
            node_name = self._node_stack.pop()
            for node in reversed(self.trace.nodes):
                if node.name == node_name and node.end_time is None:
                    node.end_time = time.time()
                    node.duration = node.end_time - node.start_time
                    node.status = "completed"

                    # æ•è·èŠ‚ç‚¹è¾“å‡º
                    if isinstance(outputs, dict):
                        self._capture_node_output(node_name, outputs, node)
                    break

        # æ£€æµ‹è·¯ç”±å†³ç­– (ä» outputs ä¸­æå– step å’Œ decision)
        if self.show_router_decisions and isinstance(outputs, dict):
            self._detect_router_decision(outputs, name or "unknown")

        self._current_node = self._node_stack[-1] if self._node_stack else None

    def _capture_node_output(self, node_name: str, outputs: Dict[str, Any], node: NodeExecution):
        """æ•è·èŠ‚ç‚¹è¾“å‡ºå†…å®¹"""
        node_output = NodeOutput(
            node_name=node_name,
            timestamp=time.time()
        )

        # æå–æ¶ˆæ¯å†…å®¹
        if "messages" in outputs and outputs["messages"]:
            messages = outputs["messages"]
            # è·å–æœ€åä¸€æ¡ AI æ¶ˆæ¯
            for msg in reversed(messages):
                if isinstance(msg, AIMessage):
                    content = msg.content
                    node_output.message_content = content
                    node.output_message = self._truncate(
                        content) if content else None
                    break
                elif hasattr(msg, "content"):
                    content = msg.content
                    node_output.message_content = content
                    node.output_message = self._truncate(
                        content) if content else None
                    break

        # æå–æ–¹æ¡ˆ (plan èŠ‚ç‚¹)
        if "generated_plans" in outputs:
            node_output.plans = outputs["generated_plans"]
            node.output_data = {"plans": outputs["generated_plans"]}

        # æå–æœç´¢é€‰é¡¹ (search_flight/search_hotel èŠ‚ç‚¹)
        if "realtime_options" in outputs:
            options = outputs["realtime_options"]
            node_output.options = options
            node.output_data = {"options": options}

        # æå–çŠ¶æ€æ›´æ–°
        state_keys = ["step", "destination",
                      "origin", "dates", "selected_plan_index"]
        state_updates = {k: outputs[k]
                         for k in state_keys if k in outputs and outputs[k]}
        if state_updates:
            node_output.state_updates = state_updates

        self.trace.node_outputs.append(node_output)

        # æ‰“å°èŠ‚ç‚¹è¾“å‡ºæ‘˜è¦
        if node_output.message_content:
            preview = self._truncate(node_output.message_content)
            icon = self._get_node_icon(node_name)
            self._log(LogLevel.INFO, f"   ğŸ’¬ [Output] {preview}", indent=0)

    def on_chain_error(self, error: BaseException, **kwargs: Any) -> Any:
        """å½“ Chain å‡ºé”™æ—¶è§¦å‘"""
        error_msg = str(error)
        self._log(LogLevel.ERROR, f"âŒ [Error] {error_msg}")

        # æ›´æ–°å½“å‰èŠ‚ç‚¹çŠ¶æ€
        if self.trace.nodes:
            self.trace.nodes[-1].status = "error"
            self.trace.nodes[-1].error = error_msg

        self.trace.status = "error"
        self.trace.error = error_msg
        self._emit_event(
            "error", {"error": error_msg, "node": self._current_node})

    def _detect_router_decision(self, outputs: Dict[str, Any], node_name: str):
        """æ£€æµ‹å¹¶è®°å½•è·¯ç”±å†³ç­–"""
        step = outputs.get("step")

        # å¦‚æœ step å‘ç”Ÿå˜åŒ–ï¼Œè¯´æ˜æœ‰è·¯ç”±å†³ç­–
        if step and step != self._last_step:
            # å°è¯•ä»æ¶ˆæ¯ä¸­æå–å†³ç­–ä¿¡æ¯
            decision = None
            reason = None

            if "messages" in outputs:
                for msg in reversed(outputs["messages"]):
                    if isinstance(msg, AIMessage) and hasattr(msg, "additional_kwargs"):
                        # æŸäº›åœºæ™¯ä¸‹ï¼Œè·¯ç”±å†³ç­–å¯èƒ½åœ¨ additional_kwargs ä¸­
                        pass

            # æ ¹æ® step å˜åŒ–æ¨æ–­å†³ç­–
            decision_record = RouterDecision(
                step=step,
                decision=f"{self._last_step or 'start'} â†’ {step}",
                reason=reason
            )
            self.trace.router_decisions.append(decision_record)

            self._log(
                LogLevel.INFO,
                f"ğŸš¦ [Router] Step={step} Decision={decision or 'transition'}",
                indent=0
            )
            self._emit_event("router_decision", {
                "from": self._last_step,
                "to": step,
                "reason": reason
            })

            self._last_step = step

    # ===== LLM è°ƒç”¨ç›‘æ§ =====

    def on_chat_model_start(
        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any
    ) -> Any:
        """å½“ Chat Model å¼€å§‹ç”Ÿæˆæ—¶è§¦å‘"""
        model_name = serialized.get(
            "id", ["unknown"])[-1] if serialized else "unknown"

        self._current_llm = LLMExecution(
            node_context=self._current_node or "unknown",
            start_time=time.time(),
            model=model_name
        )

        self._log(LogLevel.INFO, f"ğŸ¤– [LLM] Request Started...", indent=0)
        self._emit_event("llm_start", {
            "node": self._current_node,
            "model": model_name
        })

    def on_llm_start(
        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any
    ) -> Any:
        """å½“æ™®é€š LLM å¼€å§‹ç”Ÿæˆæ—¶è§¦å‘"""
        self.on_chat_model_start(serialized, [], **kwargs)

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:
        """å½“ LLM ç”Ÿæˆç»“æŸæ—¶è§¦å‘"""
        if self._current_llm:
            self._current_llm.end_time = time.time()
            self._current_llm.duration = self._current_llm.end_time - self._current_llm.start_time

            # æå– Token ä½¿ç”¨æƒ…å†µ
            if response.llm_output and "token_usage" in response.llm_output:
                usage = response.llm_output["token_usage"]
                self._current_llm.prompt_tokens = usage.get("prompt_tokens", 0)
                self._current_llm.completion_tokens = usage.get(
                    "completion_tokens", 0)
                self._current_llm.total_tokens = usage.get("total_tokens", 0)

                # ç´¯åŠ æ€»è®¡
                self.total_tokens["prompt"] += self._current_llm.prompt_tokens
                self.total_tokens["completion"] += self._current_llm.completion_tokens
                self.total_tokens["total"] += self._current_llm.total_tokens

            self.trace.llm_calls.append(self._current_llm)

            token_str = f"Tokens: {self._current_llm.total_tokens or 'N/A'}"
            self._log(
                LogLevel.INFO,
                f"âœ… [LLM] Completed in {self._current_llm.duration:.2f}s | {token_str}",
                indent=0
            )
            self._emit_event("llm_end", {
                "duration": self._current_llm.duration,
                "tokens": self._current_llm.total_tokens,
                "node": self._current_llm.node_context
            })

            self._current_llm = None

    def on_llm_error(self, error: BaseException, **kwargs: Any) -> Any:
        """å½“ LLM å‡ºé”™æ—¶è§¦å‘"""
        self._log(LogLevel.ERROR, f"âŒ [LLM Error] {error}")
        if self._current_llm:
            self._current_llm.end_time = time.time()
            self._current_llm.duration = self._current_llm.end_time - self._current_llm.start_time
            self.trace.llm_calls.append(self._current_llm)
            self._current_llm = None

    # ===== å·¥å…·è°ƒç”¨ç›‘æ§ =====

    def on_tool_start(
        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any
    ) -> Any:
        """å½“å·¥å…·å¼€å§‹è°ƒç”¨æ—¶è§¦å‘"""
        tool_name = serialized.get("name", "unknown_tool")

        self._current_tool = ToolExecution(
            name=tool_name,
            node_context=self._current_node or "unknown",
            start_time=time.time(),
            input_preview=self._truncate(
                input_str) if self.show_tool_io else None
        )

        self._log(LogLevel.INFO,
                  f"ğŸ› ï¸ [Tool] Call Started: {tool_name}", indent=0)

        if self.show_tool_io and self.log_level == LogLevel.DEBUG:
            self._log(LogLevel.DEBUG,
                      f"   Input: {self._current_tool.input_preview}", indent=1)

        self._emit_event("tool_start", {
            "name": tool_name,
            "node": self._current_node,
            "input": input_str[:100] if self.show_tool_io else None
        })

    def on_tool_end(self, output: str, **kwargs: Any) -> Any:
        """å½“å·¥å…·è°ƒç”¨ç»“æŸæ—¶è§¦å‘"""
        if self._current_tool:
            self._current_tool.end_time = time.time()
            self._current_tool.duration = self._current_tool.end_time - \
                self._current_tool.start_time
            self._current_tool.status = "completed"
            self._current_tool.output_preview = self._truncate(
                output) if self.show_tool_io else None

            self.trace.tool_calls.append(self._current_tool)

            self._log(
                LogLevel.INFO,
                f"ğŸ”§ [Tool] Completed: {self._current_tool.name} in {self._current_tool.duration:.2f}s",
                indent=0
            )

            if self.show_tool_io and self.log_level == LogLevel.DEBUG:
                self._log(
                    LogLevel.DEBUG, f"   Output: {self._current_tool.output_preview}", indent=1)

            self._emit_event("tool_end", {
                "name": self._current_tool.name,
                "duration": self._current_tool.duration,
                "output": output[:100] if self.show_tool_io else None
            })

            self._current_tool = None

    def on_tool_error(self, error: BaseException, **kwargs: Any) -> Any:
        """å½“å·¥å…·è°ƒç”¨å‡ºé”™æ—¶è§¦å‘"""
        error_msg = str(error)
        self._log(LogLevel.ERROR, f"âŒ [Tool Error] {error_msg}")

        if self._current_tool:
            self._current_tool.end_time = time.time()
            self._current_tool.duration = self._current_tool.end_time - \
                self._current_tool.start_time
            self._current_tool.status = "error"
            self._current_tool.error = error_msg
            self.trace.tool_calls.append(self._current_tool)
            self._current_tool = None

        self._emit_event("tool_error", {"error": error_msg})

    # ===== ç»Ÿè®¡æŠ¥å‘Š =====

    def print_summary(self, detailed: bool = True):
        """
        æ‰“å°æ‰§è¡Œæ‘˜è¦æŠ¥å‘Šã€‚

        Args:
            detailed: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        """
        if not self.trace.start_time:
            print("âš ï¸ No execution data to summarize.")
            return

        self.trace.end_time = time.time()
        total_duration = self.trace.end_time - self.trace.start_time

        print("\n" + "=" * 60)
        print("ğŸ“Š AGENT EXECUTION SUMMARY")
        print("=" * 60)

        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ†” Session: {self.trace.session_id}")
        print(f"ğŸ’¬ User Input: {self._truncate(self.trace.user_input)}")
        print(f"â±ï¸ Total Duration: {total_duration:.2f}s")
        print(f"ğŸ“Œ Status: {self.trace.status}")

        # èŠ‚ç‚¹ç»Ÿè®¡
        print(f"\nğŸ“ Nodes Executed: {len(self.trace.nodes)}")
        if detailed and self.trace.nodes:
            for node in self.trace.nodes:
                status_icon = "âœ…" if node.status == "completed" else "âŒ"
                duration_str = f"{node.duration:.2f}s" if node.duration else "N/A"
                output_preview = ""
                if node.output_message:
                    output_preview = f"\n      ğŸ’¬ {node.output_message}"
                print(
                    f"   {status_icon} {node.name}: {duration_str}{output_preview}")

        # LLM è°ƒç”¨ç»Ÿè®¡
        print(f"\nğŸ¤– LLM Calls: {len(self.trace.llm_calls)}")
        if self.trace.llm_calls:
            total_llm_time = sum(c.duration or 0 for c in self.trace.llm_calls)
            print(f"   Total LLM Time: {total_llm_time:.2f}s")
            print(f"   Total Tokens: {self.total_tokens['total']}")
            print(f"   â”œâ”€ Prompt: {self.total_tokens['prompt']}")
            print(f"   â””â”€ Completion: {self.total_tokens['completion']}")

        # å·¥å…·è°ƒç”¨ç»Ÿè®¡
        print(f"\nğŸ› ï¸ Tool Calls: {len(self.trace.tool_calls)}")
        if detailed and self.trace.tool_calls:
            for tool in self.trace.tool_calls:
                status_icon = "âœ…" if tool.status == "completed" else "âŒ"
                duration_str = f"{tool.duration:.2f}s" if tool.duration else "N/A"
                print(f"   {status_icon} {tool.name}: {duration_str}")

        # èŠ‚ç‚¹è¾“å‡ºè¯¦æƒ… (æ–°å¢)
        if detailed and self.trace.node_outputs:
            print(f"\nğŸ“ Node Outputs: {len(self.trace.node_outputs)}")
            for output in self.trace.node_outputs:
                icon = self._get_node_icon(output.node_name)
                print(f"   {icon} {output.node_name}:")
                if output.message_content:
                    # æ˜¾ç¤ºæ›´é•¿çš„å†…å®¹
                    content_preview = output.message_content[:300] + "..." if len(
                        output.message_content) > 300 else output.message_content
                    # å¤„ç†å¤šè¡Œå†…å®¹
                    lines = content_preview.split('\n')
                    for i, line in enumerate(lines[:5]):  # æœ€å¤šæ˜¾ç¤º5è¡Œ
                        prefix = "      " if i == 0 else "      "
                        print(f"{prefix}{line}")
                    if len(lines) > 5:
                        print(f"      ... ({len(lines) - 5} more lines)")
                if output.plans:
                    print(
                        f"      ğŸ“‹ Plans: {len(output.plans)} options generated")
                if output.options:
                    for key, val in output.options.items():
                        if isinstance(val, list):
                            print(f"      ğŸ” {key}: {len(val)} results")
                if output.state_updates:
                    print(f"      ğŸ”„ State: {output.state_updates}")

        # è·¯ç”±å†³ç­–
        if self.trace.router_decisions:
            print(f"\nğŸš¦ Router Decisions: {len(self.trace.router_decisions)}")
            if detailed:
                for decision in self.trace.router_decisions:
                    print(f"   â†’ {decision.decision}")

        # é”™è¯¯ä¿¡æ¯
        if self.trace.error:
            print(f"\nâŒ Error: {self.trace.error}")

        print("\n" + "=" * 60)

    def get_trace_json(self) -> str:
        """å¯¼å‡ºè¿½è¸ªæ•°æ®ä¸º JSON æ ¼å¼"""
        def serialize(obj):
            if hasattr(obj, "__dict__"):
                d = {}
                for k, v in obj.__dict__.items():
                    if not k.startswith("_"):
                        d[k] = serialize(v)
                return d
            elif isinstance(obj, list):
                return [serialize(i) for i in obj]
            elif isinstance(obj, dict):
                return {k: serialize(v) for k, v in obj.items()}
            elif isinstance(obj, Enum):
                return obj.value
            else:
                return obj

        return json.dumps(serialize(self.trace), ensure_ascii=False, indent=2)

    def reset(self):
        """é‡ç½®ç›‘æ§å™¨çŠ¶æ€ï¼Œç”¨äºæ–°çš„ä¼šè¯"""
        self.trace = WorkflowTrace(
            session_id=datetime.now().strftime("%Y%m%d_%H%M%S"),
            user_input="",
            start_time=0
        )
        self._current_node = None
        self._current_llm = None
        self._current_tool = None
        self._node_stack = []
        self._last_step = None
        self.total_tokens = {"prompt": 0, "completion": 0, "total": 0}


# ===== ä¾¿æ·å·¥å‚å‡½æ•° =====

def create_monitor(
    verbose: bool = False,
    session_id: Optional[str] = None
) -> AgentPerformanceMonitor:
    """
    åˆ›å»ºç›‘æ§å™¨çš„ä¾¿æ·å·¥å‚å‡½æ•°ã€‚

    Args:
        verbose: æ˜¯å¦å¼€å¯è¯¦ç»†æ¨¡å¼ (DEBUG çº§åˆ«)
        session_id: å¯é€‰çš„ä¼šè¯ ID

    Returns:
        é…ç½®å¥½çš„ AgentPerformanceMonitor å®ä¾‹
    """
    return AgentPerformanceMonitor(
        log_level=LogLevel.DEBUG if verbose else LogLevel.INFO,
        show_tool_io=True,
        show_router_decisions=True,
        session_id=session_id
    )

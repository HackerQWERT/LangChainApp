import asyncio
import json
from langchain_core.messages import HumanMessage


async def run_chat_stream(agent_graph, user_input: str, user_id: str = "default_user"):
    """
    é€šç”¨çš„ Agent æµå¼è¿è¡Œå™¨ã€‚
    è´Ÿè´£å°† Agent çš„æ€è€ƒè¿‡ç¨‹å’Œç»“æœæ¼‚äº®åœ°æ‰“å°åˆ°æ§åˆ¶å°ã€‚

    Args:
        agent_graph: ç¼–è¯‘å¥½çš„ LangGraph å¯¹è±¡
        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id: çº¿ç¨‹ IDï¼Œç”¨äºè®°å¿†åŠŸèƒ½
    """
    print(f"\nğŸ”µ ç”¨æˆ·({user_id}): {user_input}")
    print("ğŸŸ¢ Agent: ", end="", flush=True)

    # æ„é€ è¾“å…¥
    inputs = {
        "messages": [HumanMessage(content=user_input)],
        # user_id ä¸éœ€è¦ä¼ å…¥ stateï¼Œè€Œæ˜¯ä½œä¸º thread_id ä¼ å…¥ config
    }

    config = {"configurable": {"thread_id": user_id}}

    # Nodes that return static messages or messages not generated by a streaming LLM in the final step
    # These nodes construct AIMessage manually, so we need to capture their output at on_chain_end
    STATIC_MESSAGE_NODES = {
        "search_flight",
        "select_flight",
        "pay_flight",
        "search_hotel",
        "select_hotel",
        "pay_hotel",
        "summary",
        "check_weather",
        "side_chat"
    }

    try:
        # ä½¿ç”¨ astream_events v2 API è·å–ç»†ç²’åº¦çš„æµå¼äº‹ä»¶
        async for event in agent_graph.astream_events(inputs, version="v2", config=config):
            kind = event["event"]

            # 1. æ•è· LLM çš„æ–‡æœ¬æµ (on_chat_model_stream)
            # è¿™æ˜¯ LLM ç”Ÿæˆå›å¤çš„è¿‡ç¨‹
            if kind == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if hasattr(chunk, "content") and chunk.content:
                    print(chunk.content, end="", flush=True)

            # 2. æ•è·å·¥å…·è°ƒç”¨å¼€å§‹ (on_tool_start)
            # ç”¨äºæ˜¾ç¤ºç³»ç»Ÿæ­£åœ¨åšä»€ä¹ˆï¼Œå¢åŠ äº¤äº’æ„Ÿ
            elif kind == "on_tool_start":
                print(
                    f"\n   âš™ï¸  [ç³»ç»Ÿè°ƒç”¨å·¥å…·]: {event['name']} ... ", end="", flush=True)

            # 3. æ•è·å·¥å…·è°ƒç”¨ç»“æŸ (on_tool_end)
            elif kind == "on_tool_end":
                print("å®Œæˆã€‚", end="\nğŸŸ¢ Agent: ", flush=True)

            # 4. Capture output from nodes that don't stream via LLM
            elif kind == "on_chain_end":
                node_name = event["name"]
                if node_name in STATIC_MESSAGE_NODES:
                    output = event["data"].get("output")
                    if output and isinstance(output, dict) and "messages" in output:
                        messages = output["messages"]
                        if messages:
                            last_msg = messages[-1]
                            content = last_msg.content if hasattr(
                                last_msg, "content") else last_msg.get("content")
                            if content:
                                print(content, end="", flush=True)

    except Exception as e:
        print(f"\nâŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    print("\n" + "-" * 60)


async def run_monitor_stream(
    agent_graph,
    user_input: str,
    user_id: str = "default_user",
    verbose: bool = False,
    show_summary: bool = True
):
    """
    æ€§èƒ½ç›‘æ§ä¸“ç”¨ Runnerã€‚
    ä½¿ç”¨ astream_events API å®ç°æ›´å¯é çš„èŠ‚ç‚¹çº§åˆ«ç›‘æ§ã€‚

    Args:
        agent_graph: ç¼–è¯‘å¥½çš„ LangGraph å¯¹è±¡
        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id: çº¿ç¨‹ IDï¼Œç”¨äºè®°å¿†åŠŸèƒ½
        verbose: æ˜¯å¦å¼€å¯è¯¦ç»†æ¨¡å¼ (DEBUG çº§åˆ«æ—¥å¿—)
        show_summary: æ˜¯å¦åœ¨ç»“æŸåæ‰“å°æ‘˜è¦æŠ¥å‘Š
    """
    from app.infras.evaluate.evaluate_agent import (
        AgentPerformanceMonitor, LogLevel, NodeExecution, NodeOutput
    )
    import time

    print(f"\nğŸ”µ [Monitor] User({user_id}): {user_input}")
    print("ğŸš€ [Monitor] Agent Workflow Started")

    # åˆ›å»ºç›‘æ§å™¨ (ä½†ä¸»è¦é€šè¿‡ astream_events æ”¶é›†æ•°æ®)
    monitor = AgentPerformanceMonitor(
        log_level=LogLevel.DEBUG if verbose else LogLevel.INFO,
        show_tool_io=True,
        show_router_decisions=True,
        session_id=user_id
    )
    monitor.trace.user_input = user_input
    monitor.trace.start_time = time.time()

    # æ„é€ é…ç½®
    config = {
        "configurable": {"thread_id": user_id},
        "callbacks": [monitor]  # ä»ç„¶ä¿ç•™ callback ç”¨äº LLM/Tool ç›‘æ§
    }

    # å·²çŸ¥èŠ‚ç‚¹åˆ—è¡¨
    KNOWN_NODES = {
        "intent_router", "collect", "plan", "search_flight", "select_flight",
        "pay_flight", "search_hotel", "select_hotel", "pay_hotel",
        "summary", "check_weather", "side_chat", "guide"
    }

    # èŠ‚ç‚¹å›¾æ ‡æ˜ å°„
    NODE_ICONS = {
        "collect": "ğŸ“‹", "intent_router": "ğŸš¦", "plan": "ğŸ“",
        "search_flight": "âœˆï¸", "search_hotel": "ğŸ¨",
        "select_flight": "ğŸ«", "select_hotel": "ğŸ›ï¸",
        "pay_flight": "ğŸ’³", "pay_hotel": "ğŸ’°",
        "check_weather": "ğŸŒ¤ï¸", "summary": "ğŸ“Š",
        "side_chat": "ğŸ’¬", "guide": "ğŸ—ºï¸"
    }

    # èŠ‚ç‚¹æ‰§è¡Œè¿½è¸ª
    node_start_times = {}
    final_response = None
    all_messages = []  # æ”¶é›†æ‰€æœ‰æ¶ˆæ¯ç”¨äºç¡®å®šæœ€ç»ˆå›å¤

    try:
        inputs = {"messages": [HumanMessage(content=user_input)]}

        async for event in agent_graph.astream_events(inputs, version="v2", config=config):
            kind = event["event"]
            node_name = event.get("name", "")

            # === è°ƒè¯•æ¨¡å¼: æ‰“å°æ‰€æœ‰äº‹ä»¶ ===
            if verbose and kind not in ["on_chat_model_stream"]:
                print(f"   [DEBUG] Event: {kind} | Name: {node_name}")

            # === èŠ‚ç‚¹å¼€å§‹ ===
            if kind == "on_chain_start" and node_name in KNOWN_NODES:
                node_start_times[node_name] = time.time()
                icon = NODE_ICONS.get(node_name, "ğŸ“")
                print(f"{icon} [Node] Entering: {node_name}")

                # è®°å½•åˆ° trace
                node_exec = NodeExecution(
                    name=node_name,
                    start_time=node_start_times[node_name]
                )
                monitor.trace.nodes.append(node_exec)

            # === èŠ‚ç‚¹ç»“æŸ (å…³é”®: æ•è·è¾“å‡º) ===
            elif kind == "on_chain_end" and node_name in KNOWN_NODES:
                duration = time.time() - node_start_times.get(node_name, time.time())
                icon = NODE_ICONS.get(node_name, "ğŸ“")
                print(f"   âœ… [{node_name}] Completed in {duration:.2f}s")

                # æ›´æ–°èŠ‚ç‚¹è®°å½•
                for node in reversed(monitor.trace.nodes):
                    if node.name == node_name and node.end_time is None:
                        node.end_time = time.time()
                        node.duration = duration
                        node.status = "completed"
                        break

                # æå–èŠ‚ç‚¹è¾“å‡º
                output = event["data"].get("output")
                if output and isinstance(output, dict):
                    node_output = NodeOutput(
                        node_name=node_name,
                        timestamp=time.time()
                    )

                    has_content = False

                    # æå–æ¶ˆæ¯å†…å®¹ (å¤šç§æ ¼å¼å…¼å®¹)
                    if "messages" in output and output["messages"]:
                        msgs = output["messages"]
                        last_msg = msgs[-1]

                        # å°è¯•å¤šç§æ–¹å¼æå–å†…å®¹
                        content = None
                        if hasattr(last_msg, "content"):
                            content = last_msg.content
                        elif isinstance(last_msg, dict):
                            content = last_msg.get("content", "")

                        if content:
                            has_content = True
                            node_output.message_content = content
                            final_response = content  # æ›´æ–°æœ€ç»ˆå›å¤
                            # æ˜¾ç¤ºè¾“å‡ºé¢„è§ˆ
                            preview = content[:150] + \
                                "..." if len(content) > 150 else content
                            print(f"   ğŸ’¬ [Output] {preview}")

                            # æ›´æ–°èŠ‚ç‚¹è®°å½•
                            for node in reversed(monitor.trace.nodes):
                                if node.name == node_name:
                                    node.output_message = preview
                                    break

                    # æå–æ–¹æ¡ˆ
                    if "generated_plans" in output:
                        has_content = True
                        node_output.plans = output["generated_plans"]
                        print(
                            f"   ğŸ“‹ [Plans] {len(output['generated_plans'])} options generated")

                    # æå–æœç´¢é€‰é¡¹
                    if "realtime_options" in output:
                        has_content = True
                        options = output["realtime_options"]
                        node_output.options = options
                        for key, val in options.items():
                            if isinstance(val, list) and val:
                                print(f"   ğŸ” [{key}] {len(val)} results found")

                    # æå–çŠ¶æ€æ›´æ–°
                    state_keys = ["step", "destination", "origin", "dates"]
                    state_updates = {
                        k: output[k] for k in state_keys if k in output and output[k]}
                    if state_updates:
                        node_output.state_updates = state_updates
                        print(f"   ğŸ”„ [State] {state_updates}")

                    # å§‹ç»ˆè®°å½•èŠ‚ç‚¹è¾“å‡º (å³ä½¿ä¸ºç©ºï¼Œä¹Ÿæ–¹ä¾¿è°ƒè¯•)
                    monitor.trace.node_outputs.append(node_output)

                    # è°ƒè¯•: å¦‚æœèŠ‚ç‚¹æ²¡æœ‰æ•è·åˆ°å†…å®¹ï¼Œæ‰“å°åŸå§‹è¾“å‡ºå¸®åŠ©è¯Šæ–­
                    if not has_content and verbose:
                        print(
                            f"   âš ï¸ [Debug] Raw output keys: {list(output.keys())}")

            # === LangGraph å›¾å®Œæˆäº‹ä»¶ (ç”¨äºæ•è·æœ€ç»ˆçŠ¶æ€) ===
            elif kind == "on_chain_end" and node_name == "LangGraph":
                output = event["data"].get("output")
                if output and isinstance(output, dict) and "messages" in output:
                    msgs = output["messages"]
                    if msgs:
                        # è·å–æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºæœ€ç»ˆå›å¤
                        last_msg = msgs[-1]
                        content = getattr(last_msg, "content", None) or (
                            last_msg.get("content", "") if isinstance(
                                last_msg, dict) else ""
                        )
                        if content:
                            final_response = content
                            all_messages.append(content)
                            if verbose:
                                print(
                                    f"   [DEBUG] LangGraph final message captured")

        # æ›´æ–°è¿½è¸ªçŠ¶æ€
        monitor.trace.status = "completed"
        monitor.trace.final_response = final_response

        if final_response:
            print(f"\nğŸŸ¢ æœ€ç»ˆå›å¤: {final_response}")
        else:
            # å°è¯•ä»æœ€åä¸€ä¸ªæœ‰å†…å®¹çš„èŠ‚ç‚¹è¾“å‡ºä¸­è·å–
            for node_out in reversed(monitor.trace.node_outputs):
                if node_out.message_content:
                    final_response = node_out.message_content
                    monitor.trace.final_response = final_response
                    print(f"\nğŸŸ¢ æœ€ç»ˆå›å¤: {final_response}")
                    break
            else:
                print(f"\nâš ï¸ æœªæ•è·åˆ°æœ€ç»ˆå›å¤")

    except Exception as e:
        monitor.trace.status = "error"
        monitor.trace.error = str(e)
        print(f"\nâŒ è¿è¡Œé”™è¯¯: {e}")

    # æ‰“å°è¯¦ç»†æ‘˜è¦
    if show_summary:
        monitor.print_summary(detailed=True)

    print("\n" + "-" * 60)

    return monitor  # è¿”å›ç›‘æ§å™¨å®ä¾‹ï¼Œæ–¹ä¾¿è¿›ä¸€æ­¥åˆ†æ


async def sse_chat_stream(agent_graph, input_payload: dict, config: dict):
    """
    SSE (Server-Sent Events) ç”Ÿæˆå™¨ã€‚
    å®ç°äº†æ˜ç¡®çš„å‰åç«¯é€šä¿¡åè®® (Protocol Adapter)ã€‚

    Protocol Definition:
    - event: message  -> èŠå¤©æ°”æ³¡æ–‡æœ¬ (data: {content: "...", is_stream: bool})
    - event: control  -> å‰ç«¯äº¤äº’ç»„ä»¶ (data: {type: "select_plan", options: [...]})
    - event: status   -> çŠ¶æ€/Loadingæç¤º (data: {content: "..."})
    - event: error    -> é”™è¯¯ä¿¡æ¯ (data: {message: "..."})
    """

    # --- è¾…åŠ©å‡½æ•°: ç»Ÿä¸€ SSE æ ¼å¼ ---
    def create_event(event_type: str, payload: dict):
        # ensure_ascii=False ç¡®ä¿ä¸­æ–‡ä¸è¢«è½¬ä¹‰ä¸º \uXXXX
        return f"event: {event_type}\ndata: {json.dumps(payload, ensure_ascii=False)}\n\n"

    # --- é…ç½®: å…è®¸æµå¼è¾“å‡ºæ–‡æœ¬çš„èŠ‚ç‚¹ ---
    # è¿™äº›èŠ‚ç‚¹çš„ LLM è¾“å‡ºæ˜¯çº¯æ–‡æœ¬ï¼Œé€‚åˆç›´æ¥æ‰“å­—æœºå±•ç¤º
    # æ³¨æ„: å¦‚æœèŠ‚ç‚¹ä½¿ç”¨ invoke/ainvoke è€Œé streamï¼Œåˆ™ä¸ä¼šè§¦å‘ on_chat_model_stream
    # ä¸ºäº†ç¨³å®šæ€§ï¼Œæš‚æ—¶å…³é—­æµå¼ï¼Œç»Ÿä¸€ä½¿ç”¨ on_chain_end è¾“å‡º
    ALLOW_STREAMING_NODES = set()

    try:
        # ç›‘å¬ LangGraph çš„ç»†ç²’åº¦äº‹ä»¶
        async for event in agent_graph.astream_events(input_payload, version="v2", config=config):
            kind = event["event"]
            node_name = event.get("name", "")

            # --- 1. çŠ¶æ€åé¦ˆ (Status Feedback) ---
            # ç›®çš„: ç¼“è§£ç”¨æˆ·ç­‰å¾…ç„¦è™‘ï¼Œæ˜¾ç¤ºç³»ç»Ÿå½“å‰åŠ¨ä½œ
            if kind == "on_chain_start":
                if node_name in ["collect", "plan", "search_flight", "search_hotel"]:
                    yield create_event("status", {"content": "ğŸ¤” æ­£åœ¨æ€è€ƒ...", "node": node_name})

            elif kind == "on_tool_start" and not node_name.startswith("_"):
                yield create_event("status", {"content": f"âš™ï¸ è°ƒç”¨å·¥å…·: {node_name}...", "node": node_name})

            # --- 2. å®æ—¶æ–‡æœ¬æµ (Real-time Text Streaming) ---
            # ç›®çš„: æä¾›æ‰“å­—æœºæ•ˆæœã€‚ä»…å¯¹ç™½åå•èŠ‚ç‚¹å¼€æ”¾ï¼Œé˜²æ­¢ JSON æºç æ³„éœ²ã€‚
            elif kind == "on_chat_model_stream":
                if node_name in ALLOW_STREAMING_NODES:
                    chunk = event["data"]["chunk"]
                    if hasattr(chunk, "content") and chunk.content:
                        yield create_event("message", {"content": chunk.content, "is_stream": True})

            # --- 3. èŠ‚ç‚¹ç»“æœå¤„ç† (Node Result Processing) ---
            # ç›®çš„: èŠ‚ç‚¹æ‰§è¡Œç»“æŸåï¼Œæ ¹æ®èŠ‚ç‚¹ç±»å‹ï¼Œå†³å®šå‘é€ä»€ä¹ˆç»“æ„åŒ–æ•°æ®ç»™å‰ç«¯
            elif kind == "on_chain_end":
                output = event["data"].get("output")
                if not output or not isinstance(output, dict):
                    continue

                # === ç­–ç•¥ A: æ–¹æ¡ˆç”ŸæˆèŠ‚ç‚¹ ===
                if node_name == "plan":
                    # 1. å‘é€äº¤äº’æŒ‡ä»¤: å¼¹å‡ºæ–¹æ¡ˆé€‰æ‹©å¡ç‰‡
                    plans = output.get("generated_plans", [])
                    if plans:
                        yield create_event("control", {"type": "select_plan", "options": plans})
                    # 2. å‘é€æ–‡æœ¬å›å¤
                    if msgs := output.get("messages"):
                        yield create_event("message", {"content": msgs[-1].content, "is_stream": False})

                # === ç­–ç•¥ B: æœºç¥¨æœç´¢èŠ‚ç‚¹ ===
                elif node_name == "search_flight":
                    options = output.get(
                        "realtime_options", {}).get("flights", [])
                    if isinstance(options, list) and options and "error" not in options[0]:
                        yield create_event("control", {"type": "select_flight", "options": options})
                    if msgs := output.get("messages"):
                        yield create_event("message", {"content": msgs[-1].content, "is_stream": False})

                # === ç­–ç•¥ C: é…’åº—æœç´¢èŠ‚ç‚¹ ===
                elif node_name == "search_hotel":
                    options = output.get(
                        "realtime_options", {}).get("hotels", [])
                    if isinstance(options, list) and options and "error" not in options[0]:
                        yield create_event("control", {"type": "select_hotel", "options": options})
                    if msgs := output.get("messages"):
                        yield create_event("message", {"content": msgs[-1].content, "is_stream": False})

                # === ç­–ç•¥ D: æ™®é€šæ–‡æœ¬èŠ‚ç‚¹ (Collect, Pay, Weather, Summary, SideChat) ===
                # è¿™äº›èŠ‚ç‚¹é€šå¸¸è¾“å‡ºè¾ƒçŸ­çš„ç¡®è®¤ä¿¡æ¯æˆ– JSON è§£æåçš„æ–‡æœ¬
                elif node_name in ["collect", "pay_flight", "pay_hotel", "check_weather", "select_flight", "select_hotel", "guide", "summary", "side_chat"]:
                    if msgs := output.get("messages"):
                        content = msgs[-1].content
                        if content:
                            yield create_event("message", {"content": content, "is_stream": False})

                # === ç­–ç•¥ E: æ‹¦æˆªèŠ‚ç‚¹ ===
                elif node_name == "block":
                    if msgs := output.get("messages"):
                        content = msgs[-1].content
                        if content:
                            yield create_event("control", {"type": "blocked", "reason": output.get("risk_reason", "æ“ä½œè¢«æ‹¦æˆª")})
                            yield create_event("message", {"content": content, "is_stream": False})

    except Exception as e:
        yield create_event("error", {"message": str(e)})

import asyncio
from langchain_core.messages import HumanMessage


async def run_chat_stream(agent_graph, user_input: str, user_id: str = "default_user"):
    """
    é€šç”¨çš„ Agent æµå¼è¿è¡Œå™¨ã€‚
    è´Ÿè´£å°† Agent çš„æ€è€ƒè¿‡ç¨‹å’Œç»“æœæ¼‚äº®åœ°æ‰“å°åˆ°æ§åˆ¶å°ã€‚

    Args:
        agent_graph: ç¼–è¯‘å¥½çš„ LangGraph å¯¹è±¡
        user_input: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
        user_id: çº¿ç¨‹ IDï¼Œç”¨äºè®°å¿†åŠŸèƒ½
    """
    print(f"\nğŸ”µ ç”¨æˆ·({user_id}): {user_input}")
    print("ğŸŸ¢ Agent: ", end="", flush=True)

    # æ„é€ è¾“å…¥
    inputs = {
        "messages": [HumanMessage(content=user_input)],
        # user_id ä¸éœ€è¦ä¼ å…¥ stateï¼Œè€Œæ˜¯ä½œä¸º thread_id ä¼ å…¥ config
    }

    config = {"configurable": {"thread_id": user_id}}

    # Nodes that return static messages or messages not generated by a streaming LLM in the final step
    # These nodes construct AIMessage manually, so we need to capture their output at on_chain_end
    STATIC_MESSAGE_NODES = {
        "search_flight",
        "select_flight",
        "pay_flight",
        "search_hotel",
        "select_hotel",
        "pay_hotel",
        "summary",
        "check_weather",
        "side_chat"
    }

    try:
        # ä½¿ç”¨ astream_events v2 API è·å–ç»†ç²’åº¦çš„æµå¼äº‹ä»¶
        async for event in agent_graph.astream_events(inputs, version="v2", config=config):
            kind = event["event"]

            # 1. æ•è· LLM çš„æ–‡æœ¬æµ (on_chat_model_stream)
            # è¿™æ˜¯ LLM ç”Ÿæˆå›å¤çš„è¿‡ç¨‹
            if kind == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if hasattr(chunk, "content") and chunk.content:
                    print(chunk.content, end="", flush=True)

            # 2. æ•è·å·¥å…·è°ƒç”¨å¼€å§‹ (on_tool_start)
            # ç”¨äºæ˜¾ç¤ºç³»ç»Ÿæ­£åœ¨åšä»€ä¹ˆï¼Œå¢åŠ äº¤äº’æ„Ÿ
            elif kind == "on_tool_start":
                print(
                    f"\n   âš™ï¸  [ç³»ç»Ÿè°ƒç”¨å·¥å…·]: {event['name']} ... ", end="", flush=True)

            # 3. æ•è·å·¥å…·è°ƒç”¨ç»“æŸ (on_tool_end)
            elif kind == "on_tool_end":
                print("å®Œæˆã€‚", end="\nğŸŸ¢ Agent: ", flush=True)

            # 4. Capture output from nodes that don't stream via LLM
            elif kind == "on_chain_end":
                node_name = event["name"]
                if node_name in STATIC_MESSAGE_NODES:
                    output = event["data"].get("output")
                    if output and isinstance(output, dict) and "messages" in output:
                        messages = output["messages"]
                        if messages:
                            last_msg = messages[-1]
                            content = last_msg.content if hasattr(
                                last_msg, "content") else last_msg.get("content")
                            if content:
                                print(content, end="", flush=True)

    except Exception as e:
        print(f"\nâŒ è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    print("\n" + "-" * 60)
